import os
import asyncio
from tavily import TavilyClient
from openai import OpenAI


def run_tavily_search(key, query):
    # í™˜ê²½ë³€ìˆ˜ì— API í‚¤ ë„£ê¸°: export TAVILY_API_KEY="tvly-..." (mac/linux)
    client = TavilyClient(api_key=key)

    res = client.search(
        query=query,
        search_depth="advanced",  # "basic" | "advanced"
        max_results=8,  # ê°€ì ¸ì˜¬ ë¬¸ì„œ ìˆ˜
        include_answer=True,  # ê²€ìƒ‰ê²°ê³¼ ê¸°ë°˜ ìš”ì•½(LLM ìƒì„±)
        include_images=False,  # ì´ë¯¸ì§€ í•„ìš”ì‹œ True
        include_raw_content=False,  # ì›ë¬¸ ì „ë¬¸ í¬í•¨ ì—¬ë¶€
        include_image_descriptions=False,
        # days=30,  # ìµœê·¼ 30ì¼ë¡œ ì œí•œ(ì˜µì…˜)
        include_domains=[],  # í¬í•¨í•  ë„ë©”ì¸ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸
        exclude_domains=[],  # ì œì™¸í•  ë„ë©”ì¸ ë¸”ë™ë¦¬ìŠ¤íŠ¸
    )
    # print(res)
    return res


# def pretty_tavily_result(result):
#     print(f"ğŸ•’ Response time: {result.get('response_time', 'N/A')}s")
#     print(f"ğŸ” Query: {result.get('query')}")
#     print(f"ğŸ’¬ Answer: {result.get('answer')}\n")

#     print("ğŸ“„ Top Search Results:")
#     for i, r in enumerate(result.get("results", []), 1):
#         print(f"{i}. [{r['title']}]({r['url']})")
#         print(f"   â†³ {r['content'][:200]}...")
#         print(f"   (score={r.get('score', 'N/A')})\n")


def generate_answer_with_openai(query, tavily_result, openai_key):
    """
    Tavily ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ OpenAIë¥¼ ì‚¬ìš©í•´ ìƒì„¸í•œ ë‹µë³€ ìƒì„±
    """
    # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (í™˜ê²½ë³€ìˆ˜ì—ì„œ OPENAI_API_KEY ìë™ ë¡œë“œ)
    openai_client = OpenAI(api_key=openai_key)

    # Tavily ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
    context = ""
    for i, result in enumerate(tavily_result.get("results", []), 1):
        context += f"\n[ì¶œì²˜ {i}] {result['title']}\n"
        context += f"URL: {result['url']}\n"
        context += f"ë‚´ìš©: {result['content']}\n"
        context += f"ê´€ë ¨ë„ ì ìˆ˜: {result.get('score', 'N/A')}\n"
        context += "-" * 80 + "\n"

    # Tavilyê°€ ì œê³µí•˜ëŠ” ìš”ì•½ì´ ìˆë‹¤ë©´ ì¶”ê°€
    if tavily_result.get("answer"):
        context += f"\n[Tavily ìë™ ìš”ì•½]\n{tavily_result['answer']}\n"

    # OpenAIì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    system_prompt = """ë‹¹ì‹ ì€ ê¸°ìˆ  ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì œê³µëœ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  êµ¬ì¡°í™”ëœ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”."""

    user_prompt = f"""ë‹¤ìŒì€ ê²€ìƒ‰ëœ ìë£Œì…ë‹ˆë‹¤:

{context}

ìœ„ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ìƒì„¸í•œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:
{query}

ê²€ìƒ‰ ê²°ê³¼ì— í•´ë‹¹ ì •ë³´ê°€ ì¶©ë¶„í•˜ì§€ ì•Šë‹¤ë©´, ê·¸ ì ì„ ëª…ì‹œí•˜ê³  ê°€ëŠ¥í•œ ë²”ìœ„ ë‚´ì—ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”. ì ˆëŒ€ ê²€ìƒ‰ëœ ë‚´ìš©ì„ ì œì™¸í•œ ë‹¤ë¥¸ ë‚´ìš©ì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”
ê·¸ë¦¬ê³  ë§¨ ì•„ë˜ì—ëŠ” ì›¹ê²€ìƒ‰ ì¶œì²˜ ë§í¬ ì •ë³´ë¥¼ ëª…ì‹œí•´ì£¼ì„¸ìš”. 
"""

    print("\n" + "=" * 80)
    print(" OpenAIë¡œ ë‹µë³€ ìƒì„± ì¤‘...")
    print("=" * 80 + "\n")

    # OpenAI API í˜¸ì¶œ
    response = openai_client.chat.completions.create(
        model="gpt-4o",  # ë˜ëŠ” "gpt-4o-mini", "gpt-4-turbo"
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        temperature=0.7,
        max_tokens=4000,
    )

    generated_answer = response.choices[0].message.content

    print(" ìƒì„±ëœ ë‹µë³€:")
    print("=" * 80)
    print(generated_answer)
    return generated_answer


# OpenAIë¡œ ë‹µë³€ ìƒì„±
if __name__ == "__main__":

    tavily_key = "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"
    openai_key = "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"

    query = "ìš°ë¦¬ì€í–‰ ë¹„ì •í˜• ë°ì´í„° ìì‚°í™” ì‹œìŠ¤í…œì—ì„œ ê²€ìƒ‰ ìš”ì²­ì—ì„œ ì„ë² ë”©Â·ì¸ë±ì‹±Â·ê²°ê³¼ ì œê³µê¹Œì§€ì˜ í”„ë¡œì„¸ìŠ¤ê°€ 2022ë…„ê³¼ 2024ë…„ ì œì•ˆì„œì—ì„œ ê°ê° ì–´ë–»ê²Œ êµ¬ì„±ì´ ë˜ì—ˆê³  ì–´ë–»ê²Œ ë³€í™”í–ˆëŠ”ê°€?"
    res = run_tavily_search(tavily_key, query)
    answer = generate_answer_with_openai(query, res, openai_key)
